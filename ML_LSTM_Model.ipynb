{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas matplotlib scikit-learn scipy tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Activation \n",
    "*if GPU available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.dates as mdates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind =pd.read_csv('https://drive.google.com/uc?id=1sbFAHLYmyOBhHZ8yHqct2yhViRRV9r3u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 407140 entries, 0 to 407139\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   TimeStamp         407140 non-null  datetime64[ns]\n",
      " 1   WindSpeed_Mean    407140 non-null  float64       \n",
      " 2   VoltL1_Mean       407140 non-null  float64       \n",
      " 3   VoltL2_Mean       407140 non-null  float64       \n",
      " 4   VoltL3_Mean       407140 non-null  float64       \n",
      " 5   ActivePower_Mean  407140 non-null  float64       \n",
      " 6   CurrentL1_Mean    407140 non-null  float64       \n",
      " 7   CurrentL2_Mean    407140 non-null  float64       \n",
      " 8   CurrentL3_Mean    407140 non-null  float64       \n",
      " 9   PowerFactor_Mean  407140 non-null  float64       \n",
      " 10  Frequency_Mean    407140 non-null  float64       \n",
      " 11  WindSpeed_Bin     407140 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(11)\n",
      "memory usage: 37.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>WindSpeed_Mean</th>\n",
       "      <th>VoltL1_Mean</th>\n",
       "      <th>VoltL2_Mean</th>\n",
       "      <th>VoltL3_Mean</th>\n",
       "      <th>ActivePower_Mean</th>\n",
       "      <th>CurrentL1_Mean</th>\n",
       "      <th>CurrentL2_Mean</th>\n",
       "      <th>CurrentL3_Mean</th>\n",
       "      <th>PowerFactor_Mean</th>\n",
       "      <th>Frequency_Mean</th>\n",
       "      <th>WindSpeed_Bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>6.785655</td>\n",
       "      <td>392.817993</td>\n",
       "      <td>393.226410</td>\n",
       "      <td>392.404510</td>\n",
       "      <td>521.591797</td>\n",
       "      <td>588.076599</td>\n",
       "      <td>588.153687</td>\n",
       "      <td>588.288208</td>\n",
       "      <td>0.735540</td>\n",
       "      <td>50.030361</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 00:10:00</td>\n",
       "      <td>6.085925</td>\n",
       "      <td>392.750610</td>\n",
       "      <td>393.095093</td>\n",
       "      <td>392.344208</td>\n",
       "      <td>352.522308</td>\n",
       "      <td>491.073486</td>\n",
       "      <td>491.226807</td>\n",
       "      <td>491.788910</td>\n",
       "      <td>0.564326</td>\n",
       "      <td>49.993130</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 00:20:00</td>\n",
       "      <td>5.824687</td>\n",
       "      <td>393.093109</td>\n",
       "      <td>393.321503</td>\n",
       "      <td>392.433807</td>\n",
       "      <td>294.078888</td>\n",
       "      <td>472.158508</td>\n",
       "      <td>472.661804</td>\n",
       "      <td>472.771088</td>\n",
       "      <td>0.512421</td>\n",
       "      <td>49.952190</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 00:30:00</td>\n",
       "      <td>7.100197</td>\n",
       "      <td>393.863586</td>\n",
       "      <td>394.046600</td>\n",
       "      <td>393.028595</td>\n",
       "      <td>587.302795</td>\n",
       "      <td>674.494019</td>\n",
       "      <td>674.627991</td>\n",
       "      <td>674.614319</td>\n",
       "      <td>0.705167</td>\n",
       "      <td>49.975941</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 00:40:00</td>\n",
       "      <td>8.232280</td>\n",
       "      <td>394.487793</td>\n",
       "      <td>394.654785</td>\n",
       "      <td>393.713715</td>\n",
       "      <td>881.533325</td>\n",
       "      <td>866.352600</td>\n",
       "      <td>866.474121</td>\n",
       "      <td>865.911682</td>\n",
       "      <td>0.840677</td>\n",
       "      <td>49.930500</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TimeStamp  WindSpeed_Mean  VoltL1_Mean  VoltL2_Mean  VoltL3_Mean  \\\n",
       "0 2016-01-01 00:00:00        6.785655   392.817993   393.226410   392.404510   \n",
       "1 2016-01-01 00:10:00        6.085925   392.750610   393.095093   392.344208   \n",
       "2 2016-01-01 00:20:00        5.824687   393.093109   393.321503   392.433807   \n",
       "3 2016-01-01 00:30:00        7.100197   393.863586   394.046600   393.028595   \n",
       "4 2016-01-01 00:40:00        8.232280   394.487793   394.654785   393.713715   \n",
       "\n",
       "   ActivePower_Mean  CurrentL1_Mean  CurrentL2_Mean  CurrentL3_Mean  \\\n",
       "0        521.591797      588.076599      588.153687      588.288208   \n",
       "1        352.522308      491.073486      491.226807      491.788910   \n",
       "2        294.078888      472.158508      472.661804      472.771088   \n",
       "3        587.302795      674.494019      674.627991      674.614319   \n",
       "4        881.533325      866.352600      866.474121      865.911682   \n",
       "\n",
       "   PowerFactor_Mean  Frequency_Mean  WindSpeed_Bin  \n",
       "0          0.735540       50.030361            6.8  \n",
       "1          0.564326       49.993130            6.1  \n",
       "2          0.512421       49.952190            5.8  \n",
       "3          0.705167       49.975941            7.1  \n",
       "4          0.840677       49.930500            8.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wind['TimeStamp'] = pd.to_datetime(df_wind['TimeStamp'], format='mixed', errors ='coerce') #Change TimeStamp to datetime format\n",
    "df_wind.info()\n",
    "df_wind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cyclical Time Features \n",
    "To help the LSTM model learn daily patterns in wind turbine power generation,  cyclical time features were created from the timestamp. Specifically, the hour and minute were transformed into sine and cosine values to represent the circular nature of time — for example, hour 23 and hour 0 are numerically far apart but temporally adjacent.\n",
    "\n",
    "These sinusoidal features preserve the continuity of time and allow the model to capture periodic trends, such as changes in power output throughout the day. Without this transformation, the model might interpret 23:00 and 00:00 as completely unrelated, which could impair its ability to learn temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df, timestamp_col): \n",
    "    df['hour'] = df[timestamp_col].dt.hour\n",
    "    df['minute'] = df[timestamp_col].dt.minute\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    minutes = df['hour'] * 60 + df['minute']\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * minutes / 1440)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * minutes / 1440)\n",
    "\n",
    "    return df.drop(columns=['hour', 'minute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below class handles preprocessing for the LSTM model by scaling input features, creating sliding time windows (sequences), and reshaping the data into the required 3D format. It also supports reversing the scaling for interpretation of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPreprocessor:\n",
    "#Initialisation\n",
    "    def __init__(self, feature_cols, target_col, sequence_length=144):\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = MinMaxScaler() \n",
    "\n",
    "#Data Normalisation\n",
    "    def fit(self, df):\n",
    "        self.scaler.fit(df[self.feature_cols + [self.target_col]])\n",
    "\n",
    "#Applies Normalisation Scaler\n",
    "    def transform(self, df):\n",
    "        scaled = self.scaler.transform(df[self.feature_cols + [self.target_col]])\n",
    "        X, y = self.make_sequences(scaled)\n",
    "        return X, y\n",
    "\n",
    "#Creates the Sequence Length, the 'sliding window' for the LSTM \n",
    "    def make_sequences(self, data):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.sequence_length + 1):\n",
    "            # Inputs: rows i … i+sequence_length-1 (all features except final column is target)\n",
    "            X.append(data[i : i+self.sequence_length, :-1])\n",
    "            # Target: the power at the last row of that window (nowcast)\n",
    "            y.append(data[i + self.sequence_length - 1, -1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "#Apply on predictions - Scale Active Power back to orginal values\n",
    "    def inverse_transform(self, y_scaled):\n",
    "    # Ensure input is a 1D array\n",
    "        y_scaled = np.array(y_scaled).reshape(-1)\n",
    "\n",
    "        # Create dummy with the same number of columns used during scaling\n",
    "        dummy = np.zeros((len(y_scaled), len(self.feature_cols) + 1))\n",
    "        dummy[:, -1] = y_scaled\n",
    "\n",
    "        # Inverse transform and extract only the target column\n",
    "        return self.scaler.inverse_transform(dummy)[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom LSTM model allows flexible configuration of multiple stacked LSTM layers with optional dropout for regularization. It outputs a single value predicting the next 10-minute power value. The architecture is modular, making it easy to adjust the number of layers and hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_layers, dropout_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs) # Pass any layer kwargs, like name of model to super\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lstm_layers = []\n",
    "        for i in range(num_layers):\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            self.lstm_layers.append(   # Each LSTM layer is a keras Layer\n",
    "                tf.keras.layers.LSTM(\n",
    "                    hidden_size,\n",
    "                    return_sequences=return_seq,\n",
    "                    name=f\"lstm_{i}\"\n",
    "                )\n",
    "            )\n",
    "            if dropout_rate > 0: #make dropout rate optional to add\n",
    "                self.lstm_layers.append(\n",
    "                    tf.keras.layers.Dropout(\n",
    "                        dropout_rate,\n",
    "                        name=f\"dropout_{i}\"\n",
    "                    )\n",
    "                )\n",
    "        self.output_layer = tf.keras.layers.Dense(1, name=\"output_dense\") # Dense layer is the output layer, \n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.lstm_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function trains the custom LSTM model using the Adam optimizer and Mean Squared Error (MSE) loss. It accepts training and validation sets, runs for a specified number of epochs and batch size, and returns the training history for later analysis. shuffle=False is used to preserve the time order of sequences, which is essential in time series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with LSTM\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse') #Use adam as inital optimser and learning rate initalisor, Mean Squared Error (mse) as loss function\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=epochs, batch_size=batch_size, verbose=1, shuffle = False) #Input epochs and batch size manually, use verbose to show progress bar during training\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles model evaluation by calculating and reporting performance metrics across different dataset splits (e.g., train, validation, test). It first inversely transforms the scaled predictions using the fitted preprocessor, then computes standard regression metrics including RMSE, R², MAE, and SMAPE. Values close to zero, below a small threshold, are masked out to avoid instability in percentage-based metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationReport:\n",
    "    def __init__(self, preprocessor, X_sets, target_col):\n",
    "\n",
    "        self.preprocessor = preprocessor\n",
    "        self.X_sets = X_sets\n",
    "        self.target_col = target_col\n",
    "\n",
    "    @staticmethod \n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        eps = 1e-2\n",
    "        mask = y_true > eps\n",
    "\n",
    "        if np.sum(mask) == 0:\n",
    "            print(\"[Warning] No valid values after masking low y_true values.\")\n",
    "            return {}\n",
    "\n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]\n",
    "       \n",
    "        #Evaluation Metrics \n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mean_y = np.mean(y_true)\n",
    "        rmsepe_approx = (rmse / mean_y) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        smape = np.mean(2 * np.abs(y_pred - y_true)\n",
    "                                 / (np.abs(y_true) + np.abs(y_pred) + eps)) * 100\n",
    "\n",
    "        return {\n",
    "            'RMSE': rmse,\n",
    "            'RMSEPE_approx (%)': rmsepe_approx,\n",
    "            'R2': r2,\n",
    "            'MAE': mae ,\n",
    "            'SMAPE (%)': smape\n",
    "        }\n",
    "\n",
    "    def evaluate(self, model):\n",
    "        results = {}\n",
    "        for name, (X, y, _) in self.X_sets.items():\n",
    "            y_pred = model.predict(X, verbose=1).flatten()\n",
    "            y_true_inv = self.preprocessor.inverse_transform(y)\n",
    "            y_pred_inv = self.preprocessor.inverse_transform(y_pred)\n",
    "\n",
    "            metrics = self.calculate_metrics(y_true_inv, y_pred_inv)\n",
    "\n",
    "            print(f\"\\n=== {name} Metrics ===\")\n",
    "            print(len(y_true_inv), len(y_pred_inv))\n",
    "            for m, v in metrics.items():\n",
    "                print(f\"{m.ljust(18)}: {v:8.2f}\")\n",
    "            \n",
    "\n",
    "            results[name] = metrics\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs a grid search over hyperparameters for LSTM-based nowcasting of wind turbine power. It automates the full pipeline: data splitting, time feature creation, scaling, sequence generation, model training with early stopping, and evaluation using multiple performance metrics. For each configuration, training, validation, and test metrics are logged to a CSV file, and loss curves are saved for comparison. This setup allows for robust model selection based on empirical performance across various parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_with_grid_search(\n",
    "    df, timestamp_col, feature_cols, target_col,\n",
    "    sequence_length_options, hidden_size_options, learning_rate_options,\n",
    "    num_layers_options, batch_size_options, dropout_rate_options, epoch_options,\n",
    "    log_path='final_model_.csv', model_dir='final_model'\n",
    "):\n",
    "    def check_for_nans_and_infs(X, y, label):\n",
    "        if np.isnan(X).any() or np.isnan(y).any():\n",
    "            print(f\"[ERROR] NaNs detected in {label}\")\n",
    "        if np.isinf(X).any() or np.isinf(y).any():\n",
    "            print(f\"[ERROR] Infs detected in {label}\")\n",
    "\n",
    "#Define function for TS splitting of dataset\n",
    "    def time_series_split(df, timestamp_col, train_frac=0.7, val_frac=0.15, test_frac=0.15): \n",
    "        df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        \n",
    "        train_end = int(n * train_frac)\n",
    "        val_end = train_end + int(n * val_frac)\n",
    "        \n",
    "        train_df = df.iloc[:train_end]\n",
    "        val_df   = df.iloc[train_end:val_end]\n",
    "        test_df  = df.iloc[val_end:]\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "#Plot loss curves of training and validation loss\n",
    "    def plot_loss_curve(history, config_id): \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "        plt.title(f'Loss Curve - {config_id}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        os.makedirs('loss_plots', exist_ok=True)\n",
    "        plt.savefig(f'loss_plots/loss_curve_{config_id}.png')\n",
    "        plt.close()\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare log file\n",
    "    dummy_metrics = EvaluationReport.calculate_metrics(np.array([10.0,12.0]), np.array([10.0, 11.5]))\n",
    "    log_fields = [\n",
    "        'timestamp', 'sequence_length', 'hidden_size', 'learning_rate',\n",
    "        'dropout_rate', 'num_layers', 'batch_size', 'epochs'\n",
    "    ]\n",
    "    for split in ['Train', 'Validation', 'Test']:\n",
    "        log_fields.extend([f\"{split}_{k}\" for k in dummy_metrics.keys()])\n",
    "\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, 'w', newline='') as f:\n",
    "            csv.writer(f).writerow(log_fields)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    df = create_time_features(df, timestamp_col)\n",
    "\n",
    "    for seq_len in sequence_length_options:\n",
    "        print(f\"Generating sequences for sequence_length = {seq_len}\")\n",
    "\n",
    "        # Split DataFrame based on timestamp\n",
    "        train_df, val_df, test_df = time_series_split(df, 'TimeStamp')\n",
    "\n",
    "\n",
    "        # Apply LSTMPreprocessor\n",
    "        pre = LSTMPreprocessor(feature_cols, target_col, seq_len)\n",
    "        pre.fit(train_df)\n",
    "\n",
    "        #check for small sequences\n",
    "        if any(len(split) <= seq_len for split in [train_df, val_df, test_df]):\n",
    "            print(f\"[SKIP] Skipping config {config_id} due to insufficient rows after split.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        X_train, y_train = pre.transform(train_df)\n",
    "        X_val, y_val     = pre.transform(val_df)\n",
    "        X_test, y_test   = pre.transform(test_df)\n",
    "        ts_train = train_df[timestamp_col].iloc[seq_len:].reset_index(drop=True)\n",
    "        ts_val = val_df[timestamp_col].iloc[seq_len:].reset_index(drop=True)\n",
    "        ts_test = test_df[timestamp_col].iloc[seq_len:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "        check_for_nans_and_infs(X_train, y_train, \"Train Set\")\n",
    "        check_for_nans_and_infs(X_val, y_val, \"Validation Set\")\n",
    "\n",
    "        for h_size, lr, nl, bs, dr, ep in product(\n",
    "            hidden_size_options,\n",
    "            learning_rate_options,\n",
    "            num_layers_options,\n",
    "            batch_size_options,\n",
    "            dropout_rate_options,\n",
    "            epoch_options\n",
    "        ):\n",
    "            config_id = f\"seq{seq_len}_hid{h_size}_lr{lr}_nl{nl}_bs{bs}_dr{dr}_ep{ep}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            print(f\"Trying config: {config_id}\")\n",
    "\n",
    "            model = CustomLSTM(hidden_size=h_size, num_layers=nl, dropout_rate=dr)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=ep,\n",
    "                batch_size=bs,\n",
    "                callbacks=[\n",
    "                    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                    ModelCheckpoint(os.path.join(model_dir, f\"{config_id}.keras\"), monitor='val_loss', save_best_only=True)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            plot_loss_curve(history, config_id)\n",
    "\n",
    "            val_report = EvaluationReport(preprocessor=pre, X_sets={\"Validation\": (X_val, y_val, ts_val)}, target_col=target_col)\n",
    "            train_report = EvaluationReport(preprocessor=pre, X_sets={\"Train\": (X_train, y_train, ts_train)}, target_col=target_col)\n",
    "            test_report = EvaluationReport(preprocessor=pre, X_sets={\"Test\": (X_test, y_test, ts_test)}, target_col=target_col)\n",
    "            val_metrics = val_report.evaluate(model)[\"Validation\"]\n",
    "            train_metrics = train_report.evaluate(model)[\"Train\"]\n",
    "            test_metrics = test_report.evaluate(model)[\"Test\"]\n",
    "\n",
    "\n",
    "            with open(log_path, 'a', newline='') as f:\n",
    "                row = [\n",
    "                    datetime.now(), seq_len, h_size, lr, dr, nl, bs, ep\n",
    "                ]\n",
    "                for metrics in [train_metrics, val_metrics, test_metrics]:\n",
    "                    row.extend([metrics.get(k, None) for k in dummy_metrics.keys()])\n",
    "                csv.writer(f).writerow(row)\n",
    "\n",
    "\n",
    "    print(\"[INFO] Grid search completed. All configurations and validation metrics are logged.\")\n",
    "    print(f\"[INFO] Check the log file here: {log_path}\")\n",
    "\n",
    "    return log_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Grid Search of 30 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "\n",
    "#Define full hyper‐parameter pool\n",
    "param_pools = {\n",
    "    'sequence_length_options': [24, 96, 144, 150],\n",
    "    'hidden_size_options':     [32, 64, 128, 256],\n",
    "    'learning_rate_options':   [0.00001, 0.001, 0.0003],\n",
    "    'num_layers_options':      [1, 2, 3],\n",
    "    'batch_size_options':      [16, 32, 64],\n",
    "    'dropout_rate_options':    [0.1, 0.2, 0.3],\n",
    "    'epoch_options':           [10, 50, 150, 200, 300]\n",
    "}\n",
    "\n",
    "#Generate the full list of tuples, then samples 30 of them\n",
    "all_configs = list(product(\n",
    "    param_pools['sequence_length_options'],\n",
    "    param_pools['hidden_size_options'],\n",
    "    param_pools['learning_rate_options'],\n",
    "    param_pools['num_layers_options'],\n",
    "    param_pools['batch_size_options'],\n",
    "    param_pools['dropout_rate_options'],\n",
    "    param_pools['epoch_options'],\n",
    "))\n",
    "random.seed(42)\n",
    "sampled = random.sample(all_configs, 30)\n",
    "\n",
    "#Loop over each sampled tuple, calling existing grid search\n",
    "for (seq, hid, lr, nl, bs, dr, ep) in sampled:\n",
    "    print(f\"→ Running random config: seq={seq}, hid={hid}, lr={lr}, layers={nl}, bs={bs}, dr={dr}, ep={ep}\")\n",
    "    run_pipeline_with_grid_search(\n",
    "        df=df_wind,\n",
    "        timestamp_col='TimeStamp',\n",
    "        feature_cols=[\n",
    "            \"VoltL1_Mean\",\"VoltL2_Mean\",\"VoltL3_Mean\",\n",
    "            \"CurrentL1_Mean\",\"CurrentL2_Mean\",\"CurrentL3_Mean\",\n",
    "            \"PowerFactor_Mean\",\"Frequency_Mean\",\"WindSpeed_Mean\",\n",
    "            \"hour_sin\",\"hour_cos\",\"minute_sin\",\"minute_cos\"\n",
    "        ],\n",
    "        target_col='ActivePower_Mean',\n",
    "        sequence_length_options=[seq], \n",
    "        hidden_size_options=[hid],\n",
    "        learning_rate_options=[lr],\n",
    "        num_layers_options=[nl],\n",
    "        batch_size_options=[bs],\n",
    "        dropout_rate_options=[dr],\n",
    "        epoch_options=[ep],\n",
    "        log_path='rand_search_log_nowcast.csv',\n",
    "        model_dir='rand_saved_models_nowcast'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run & Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequences for sequence_length = 96\n",
      "Trying config: seq96_hid128_lr0.0003_nl3_bs32_dr0.2_ep100_20250510_223729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746909449.770399   15228 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 83ms/step - loss: 0.0043 - val_loss: 7.0920e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 85ms/step - loss: 1.0559e-04 - val_loss: 4.7878e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 83ms/step - loss: 5.1545e-05 - val_loss: 3.0526e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 82ms/step - loss: 3.6129e-05 - val_loss: 2.4446e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 81ms/step - loss: 2.9707e-05 - val_loss: 1.6130e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 85ms/step - loss: 2.5976e-05 - val_loss: 2.8094e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 85ms/step - loss: 2.3262e-05 - val_loss: 2.0806e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 83ms/step - loss: 2.3978e-05 - val_loss: 1.3511e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 84ms/step - loss: 2.1925e-05 - val_loss: 2.1381e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 82ms/step - loss: 2.0423e-05 - val_loss: 1.6605e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 82ms/step - loss: 1.9370e-05 - val_loss: 1.3795e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 85ms/step - loss: 1.8902e-05 - val_loss: 1.4389e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 84ms/step - loss: 1.8100e-05 - val_loss: 1.3302e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 87ms/step - loss: 1.7986e-05 - val_loss: 1.5824e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 90ms/step - loss: 1.7412e-05 - val_loss: 1.9255e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 88ms/step - loss: 1.7061e-05 - val_loss: 1.5325e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 89ms/step - loss: 1.6564e-05 - val_loss: 1.1490e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 90ms/step - loss: 1.6903e-05 - val_loss: 1.4873e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 90ms/step - loss: 1.6564e-05 - val_loss: 1.2179e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 90ms/step - loss: 1.6678e-05 - val_loss: 1.1142e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 90ms/step - loss: 1.5836e-05 - val_loss: 1.1958e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m830s\u001b[0m 93ms/step - loss: 1.4961e-05 - val_loss: 2.0883e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m829s\u001b[0m 93ms/step - loss: 1.5475e-05 - val_loss: 1.1603e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m829s\u001b[0m 93ms/step - loss: 1.4776e-05 - val_loss: 3.2889e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m832s\u001b[0m 93ms/step - loss: 1.4111e-05 - val_loss: 1.2568e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m821s\u001b[0m 92ms/step - loss: 1.4375e-05 - val_loss: 9.0884e-06\n",
      "Epoch 27/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m832s\u001b[0m 93ms/step - loss: 1.3525e-05 - val_loss: 3.8361e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 95ms/step - loss: 1.4182e-05 - val_loss: 1.7797e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m837s\u001b[0m 94ms/step - loss: 1.2890e-05 - val_loss: 1.3342e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m849s\u001b[0m 95ms/step - loss: 1.4058e-05 - val_loss: 1.0760e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m869s\u001b[0m 98ms/step - loss: 1.2304e-05 - val_loss: 9.9906e-06\n",
      "Epoch 32/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 96ms/step - loss: 1.2824e-05 - val_loss: 1.0344e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m839s\u001b[0m 94ms/step - loss: 1.2275e-05 - val_loss: 9.7755e-06\n",
      "Epoch 34/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m855s\u001b[0m 96ms/step - loss: 1.2293e-05 - val_loss: 9.7346e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m870s\u001b[0m 98ms/step - loss: 1.1835e-05 - val_loss: 1.1056e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m897s\u001b[0m 101ms/step - loss: 1.1555e-05 - val_loss: 1.0936e-05\n",
      "\u001b[1m1906/1906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 34ms/step\n",
      "\n",
      "=== Validation Metrics ===\n",
      "60976 60976\n",
      "RMSE              :     7.64\n",
      "RMSEPE_approx (%) :     0.99\n",
      "R2                :     1.00\n",
      "MAE               :     3.70\n",
      "SMAPE (%)         :     4.48\n",
      "\u001b[1m8904/8904\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 33ms/step\n",
      "\n",
      "=== Train Metrics ===\n",
      "284903 284903\n",
      "RMSE              :     8.00\n",
      "RMSEPE_approx (%) :     1.04\n",
      "R2                :     1.00\n",
      "MAE               :     4.75\n",
      "SMAPE (%)         :     5.01\n",
      "\u001b[1m1906/1906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step\n",
      "\n",
      "=== Test Metrics ===\n",
      "60976 60976\n",
      "RMSE              :     9.56\n",
      "RMSEPE_approx (%) :     1.56\n",
      "R2                :     1.00\n",
      "MAE               :     4.98\n",
      "SMAPE (%)         :    11.79\n",
      "[INFO] Grid search completed. All configurations and validation metrics are logged.\n",
      "[INFO] Check the log file here: test_grid_log_final.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_grid_log_final.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline_with_grid_search(\n",
    "    df=df_wind,\n",
    "    timestamp_col='TimeStamp',\n",
    "    feature_cols=[\"VoltL1_Mean\", \"VoltL2_Mean\", \"VoltL3_Mean\",\n",
    "    \"CurrentL1_Mean\", \"CurrentL2_Mean\", \"CurrentL3_Mean\", \"PowerFactor_Mean\",\n",
    "    \"Frequency_Mean\", \"WindSpeed_Mean\", \"hour_sin\", \"hour_cos\", \"minute_sin\", \"minute_cos\"],\n",
    "    target_col='ActivePower_Mean',\n",
    "    sequence_length_options=[96], \n",
    "    hidden_size_options=[128],\n",
    "    learning_rate_options=[0.0003],\n",
    "    num_layers_options=[3],\n",
    "    batch_size_options=[32],\n",
    "    dropout_rate_options=[0.2],\n",
    "    epoch_options=[100],\n",
    "    log_path='test_grid_log_final.csv',\n",
    "    model_dir='test_saved_model_final'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
